{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%install_ext` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\"\n",
    "    function to load data from data directory\n",
    "    \"\"\"\n",
    "    relevant = pd.read_json(data_dir + '/relevant_articles_sample.json');\n",
    "    relevant['index'] = relevant.index\n",
    "    relevant['label']=1\n",
    "    irrelevant = pd.read_json(data_dir + '/irrelevant_articles_sample.json')\n",
    "    irrelevant['index']=irrelevant.index\n",
    "    irrelevant['label']=0  \n",
    "    return (relevant,irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant,irrelevant = load_data(\"../../data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The lowly milkshake has turned into an unlikel...\n",
       "1    ANZ has moved to the front of the race that’s ...\n",
       "2    Jul. 10--Carnival Cruise Line stateroom attend...\n",
       "3    CHENNAI: R Rohit (5/43) and P Saravanan (5/46)...\n",
       "4    Donald Trump’s nominee to lead the US Fish and...\n",
       "5    SAN FRANCISCO, May 30, 2019 /PRNewswire/ -- Ca...\n",
       "6    Seventy-five years ago, President Franklin Del...\n",
       "7    GARDEN GROVE, Calif., Aug. 26, 2019 /PRNewswir...\n",
       "8    EAST RUTHERFORD, N.J. (AP) — More than two dec...\n",
       "9    Oct. 31--Ford Motor Co. has gone from beating ...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(relevant[0:10]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The acquisition complements u-blox's commitmen...\n",
       "1    Melbourne City are set to unleash 16-year-old ...\n",
       "2    So much for being a 42.5-point favorite. The n...\n",
       "3    Twentieth century slang that should NOT be bro...\n",
       "4    Breaking cycle The Government will spend $320 ...\n",
       "5    Thieves broke into 40 vehicles after dark in N...\n",
       "6    As of last month, Guilford County Schools stil...\n",
       "7    Durant limps off Kevin Durant’s comeback game ...\n",
       "8    Cubs 8 Angels 1, Dodgers 3 Diamondbacks 1, Ast...\n",
       "9    Nuggets 96 Trail Blazers 100, Raptors 92 76ers...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelevant[0:10]['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TextPreprocessor Sci-kit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharun/anaconda3/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.21.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from normalise import normalise\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" Stop words \"\"\"\n",
    "stopWords = [\"nt\",\"'ll\", \"'ve\", '1-1', 'a', \"a's\", 'able', 'about', 'above', 'abroad', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'added', 'adj', 'adopted', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'again', 'against', 'ago', 'ah', 'ahead', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'apparently', 'appear', 'appreciate', 'appropriate', 'approximately', 'are', 'area', 'areas', 'aren', \"aren't\", 'arent', 'arise', 'around', 'as', 'aside', 'ask', 'asked', 'asking', 'asks', 'associated', 'at', 'auth', 'available', 'away', 'awfully', 'b', 'back', 'backed', 'backing', 'backs', 'backward', 'backwards', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'began', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'beings', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'big', 'bill', 'biol', 'both', 'bottom', 'brief', 'briefly', 'but', 'by', 'c', \"c'mon\", \"c's\", 'ca', 'call', 'called', 'came', 'can', \"can't\", 'cannot', 'cant', 'caption', 'case', 'cases', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clear', 'clearly', 'co', 'co.', 'com', 'come', 'comes', 'computer', 'con', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'couldnt', 'course', 'cry', 'currently', 'd', 'dare', \"daren't\", 'date', 'de', 'dear', 'definitely', 'describe', 'described', 'despite', 'detail', 'did', \"didn't\", 'differ', 'different', 'differently', 'directly', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downed', 'downing', 'downs', 'downwards', 'due', 'during', 'e', 'each', 'early', 'ed', 'edu', 'effect', 'eg', 'eight', 'eighty', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'end', 'ended', 'ending', 'ends', 'enough', 'entirely', 'especially', 'et', 'et-al', 'etc', 'even', 'evenly', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'face', 'faces', 'fact', 'facts', 'fairly', 'far', 'farther', 'felt', 'few', 'fewer', 'ff', 'fifteen', 'fifth', 'fify', 'fill', 'find', 'finds', 'fire', 'first', 'five', 'fix', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forty', 'forward', 'found', 'four', 'from', 'front', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthermore', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'getting', 'give', 'given', 'gives', 'giving', 'go', 'goes', 'going', 'gone', 'good', 'goods', 'got', 'gotten', 'great', 'greater', 'greatest', 'greetings', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', \"hadn't\", 'half', 'happens', 'hardly', 'has', \"hasn't\", 'hasnt', 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'hed', 'held', 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herse', 'herself', 'hes', 'hi', 'hid', 'high', 'higher', 'highest', 'him', 'himse', 'himself', 'his', 'hither', 'home', 'hopefully', 'how', 'howbeit', 'however', 'hundred', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'id', 'ie', 'if', 'ignored', 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inasmuch', 'inc', 'inc.', 'include', 'included', 'including', 'indeed', 'index', 'indicate', 'indicated', 'indicates', 'information', 'inner', 'inside', 'insofar', 'instead', 'interest', 'interested', 'interesting', 'interests', 'into', 'invention', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'itd', 'its', 'itse', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'keys', 'kg', 'kind', 'km', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'late', 'lately', 'later', 'latest', 'latter', 'latterly', 'least', 'led', 'less', 'lest', 'let', \"let's\", 'lets', 'like', 'liked', 'likely', 'likewise', 'line', 'links', 'little', 'long', 'longer', 'longest', 'look', 'looking', 'looks', 'low', 'lower', 'ltd', 'm', 'made', 'mainly', 'make', 'makes', 'making', 'man', 'many', 'may', 'maybe', \"mayn't\", 'me', 'mean', 'means', 'meantime', 'meanwhile', 'member', 'members', 'men', 'merely', 'mg', 'might', \"mightn't\", 'mill', 'million', 'mine', 'minus', 'miss', 'ml', 'more', 'moreover', 'most', 'mostly', 'move', 'moved', 'mr', 'mrs', 'much', 'mug', 'must', \"mustn't\", 'my', 'myse', 'myself', 'n', 'na', 'name', 'namely', 'nay', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needed', 'needing', \"needn't\", 'needs', 'neither', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'newer', 'newest', 'next', 'nine', 'ninety', 'no', 'no-one', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'number', 'numbers', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'oldest', 'omitted', 'on', 'once', 'one', \"one's\", 'ones', 'only', 'onto', 'open', 'opened', 'opening', 'opens', 'opposite', 'or', 'ord', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'otherwise', 'ought', \"oughtn't\", 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'page', 'pages', 'part', 'parted', 'particular', 'particularly', 'parting', 'parts', 'past', 'per', 'perhaps', 'place', 'placed', 'places', 'please', 'plus', 'point', 'pointed', 'pointing', 'points', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'predominantly', 'present', 'presented', 'presenting', 'presents', 'presumably', 'previously', 'primarily', 'probably', 'problem', 'problems', 'promptly', 'proud', 'provided', 'provides', 'put', 'puts', 'q', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'reasonably', 'received', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'respectively', 'resulted', 'resulting', 'results', 'right', 'room', 'rooms', 'round', 'run', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sec', 'second', 'secondly', 'seconds', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'sees', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'shed', 'shes', 'should', \"shouldn't\", 'show', 'showed', 'showing', 'shown', 'showns', 'shows', 'side', 'sides', 'significant', 'significantly', 'similar', 'similarly', 'since', 'sincere', 'six', 'sixty', 'slightly', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'state', 'states', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure', 'system', 't', \"t's\", 'take', 'taken', 'taking', 'tell', 'ten', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", \"that's\", \"that've\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there'd\", \"there'll\", \"there're\", \"there's\", \"there've\", 'thereafter', 'thereby', 'thered', 'therefore', 'therein', 'thereof', 'therere', 'theres', 'thereto', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'theyd', 'theyre', 'thick', 'thin', 'thing', 'things', 'think', 'thinks', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'thou', 'though', 'thoughh', 'thought', 'thoughts', 'thousand', 'three', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'till', 'time', 'tip', 'tis', 'to', 'today', 'together', 'too', 'took', 'top', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', 'turn', 'turned', 'turning', 'turns', 'twas', 'twelve', 'twenty', 'twice', 'two', 'u', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'upwards', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'versus', 'very', 'via', 'viz', 'vol', 'vols', 'vs', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', \"wasn't\", 'way', 'ways', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'wed', 'welcome', 'well', 'wells', 'went', 'were', \"weren't\", 'what', \"what'll\", \"what's\", \"what've\", 'whatever', 'whats', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whim', 'whither', 'who', \"who'd\", \"who'll\", \"who's\", 'whod', 'whoever', 'whole', 'whom', 'whomever', 'whos', 'whose', 'why', 'widely', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'words', 'work', 'worked', 'working', 'works', 'world', 'would', \"wouldn't\", 'written', 'www', 'x', 'y', 'year', 'years', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'youd', 'young', 'younger', 'youngest', 'your', 'yourabout', 'youre', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Implement Sci-kit base classes BaseEstimated and TransformerMixin\n",
    "    This class can be used in the pipelines inside sci-kit transforms\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 variety=\"BrE\",\n",
    "                 user_abbrevs={},\n",
    "                 n_jobs=1):\n",
    "        self.variety = variety\n",
    "        self.user_abbrevs = user_abbrevs\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        \"\"\"\n",
    "        Transform raw data into cleaned data\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        partitions = 1\n",
    "        cores = mp.cpu_count()\n",
    "        if self.n_jobs <= -1:\n",
    "            partitions = cores\n",
    "        elif self.n_jobs <= 0:\n",
    "            return X_copy.apply(self._preprocess_text)\n",
    "        else:\n",
    "            partitions = min(self.n_jobs, cores)\n",
    "\n",
    "        data_split = np.array_split(X_copy, partitions)\n",
    "        pool = mp.Pool(cores)\n",
    "        data = pd.concat(pool.map(self._preprocess_part, data_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def strip_html(self,text):\n",
    "        \"\"\" Parse HTML if any\"\"\"\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "\n",
    "    def remove_between_square_brackets(self,text):\n",
    "        \"\"\" Get text between paranthesis\"\"\"\n",
    "        return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "    def denoise_text(self,text):\n",
    "        \"\"\" Denoise text, strip html\"\"\"\n",
    "        text = self.strip_html(text)\n",
    "        text = self.remove_between_square_brackets(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_non_ascii(self,words):\n",
    "        \"\"\" Remove non ascii characters \"\"\"    \n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def to_lowercase(self,words):\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_punctuation(self,words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def replace_numbers(self,words):\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        p = inflect.engine()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word.isdigit():\n",
    "                new_word = p.number_to_words(word)\n",
    "                new_words.append(new_word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def remove_stopwords(self,words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english') and word not in stopWords:\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "\n",
    "    def stem_words(self,words):\n",
    "        \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "\n",
    "    def lemmatize_verbs(self,words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "    def normalize(self,words):\n",
    "        \"\"\"\n",
    "        Normalize remove non_ascii, punctuation, converts_to_lowecase and removes stopwords\n",
    "        \"\"\"\n",
    "        words = self.remove_non_ascii(words)\n",
    "        words = self.remove_punctuation(words)\n",
    "        words = self.to_lowercase(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def stem_and_lemmatize(self,words):\n",
    "        \"\"\"\n",
    "        Lemmatizes verbs\n",
    "        \"\"\"\n",
    "        lemmas = self.lemmatize_verbs(words)\n",
    "        return lemmas\n",
    "\n",
    "    def _preprocess_part(self, part):\n",
    "        \"\"\"\n",
    "        Preprocesses chunks of text\n",
    "        \"\"\"\n",
    "        return part.apply(self._preprocess_text)\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses text\n",
    "        \"\"\"\n",
    "        sample = text\n",
    "        words = nltk.word_tokenize(sample)\n",
    "        words = self.normalize(words)\n",
    "        words = self.stem_and_lemmatize(words)\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 7 µs, total: 17 µs\n",
      "Wall time: 26.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean_data(dataframe,column_name='content',n_jobs=10):\n",
    "    \"\"\"\n",
    "    Function to clean the dataframe\n",
    "    \"\"\"\n",
    "    clean = TextPreprocessor(n_jobs=n_jobs).transform(dataframe[column_name])\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload clean data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_file(dataframe, data_dir, filename):\n",
    "    dataframe.to_json(data_dir + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_clean = clean_data(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_clean = clean_data(irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataframe_to_file(relevant_clean,\"../../data/clean/\",\"relevant_news.json\")\n",
    "upload_dataframe_to_file(irrelevant_clean,\"../../data/clean/\",\"irrelevant_news.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
