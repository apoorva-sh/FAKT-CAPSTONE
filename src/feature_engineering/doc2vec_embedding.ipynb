{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBClassifier \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(corpus, tokens_only=False):\n",
    "    '''\n",
    "    Input:\n",
    "    corpus: Refers to each document stored in a single string\n",
    "    token_only: if 'True', tags the data (used for test)\n",
    "    \n",
    "    Output:\n",
    "    Returns document text that is pre-processed and cleaned by gensim preprocess module\n",
    "    \n",
    "    '''\n",
    "    for i, line in tqdm_notebook(enumerate(corpus), total = len(corpus)):\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "            \n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \n",
    "    '''\n",
    "    Records the Start and End of an Epoch to print a message accordingly \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "             print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "            print(\"Epoch #{} end\".format(self.epoch))\n",
    "            self.epoch += 1\n",
    "            \n",
    "            \n",
    "def Doc2vec(train_corpus, test_corpus,vector_size=300, min_count=2, epochs=10):\n",
    "    '''\n",
    "    Input:\n",
    "    train_corpus: train corpus that has been preprocess by gensim preprocess module\n",
    "    test_corpus: test corpus that has been tagged and preprocessed by gensim preprocess module\n",
    "    vector_size: dimension of doc2vec embedding\n",
    "    min_count: window size for the skip_gram model\n",
    "    epochs: No: of epochs to train the model\n",
    "    \n",
    "    Output:\n",
    "    Saves the train/test embeddings \n",
    "    '''\n",
    "    \n",
    "    #Train Doc2vec model with vector size 300 for 10 epochs\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=vector_size, min_count=min_count, epochs=epochs, workers= 4,compute_loss = True)\n",
    "    model.build_vocab(train_corpus)\n",
    "    epoch_logger = EpochLogger()\n",
    "    \n",
    "    #Train model for 10 epochs\n",
    "    model.train(train_corpus,  total_examples=model.corpus_count,epochs=model.epochs, callbacks=[epoch_logger])\n",
    "    \n",
    "    #Obtain Doc2vec vectors of the train corpus\n",
    "    X_train_vector = np.array([model.docvecs[i] for i in range(len(train_corpus))])\n",
    "    \n",
    "    #Infer test corpus embeddings from the trained model\n",
    "    X_test_vector = np.array([model.infer_vector(test_corpus[i]) for i in range(len(test_corpus))])\n",
    "    \n",
    "    np.save('../../data/feature/Doc2vec_train_embeddings', X_train_vector)\n",
    "    np.save('../../data/feature/Doc2vec_test_embeddings', X_test_vector)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def main(X,Y,split=0.2):\n",
    "    '''\n",
    "    Splits the data into train/test and trains Doc2Vec model\n",
    "    The doc2vec embeddings are saved in the 'data/feature' directory\n",
    "    \n",
    "    Input:\n",
    "    X: Data to be used for doc2vec features\n",
    "    Y: corresponding labels\n",
    "    split: Test set size percentage to split\n",
    "    \n",
    "    Output:\n",
    "    Saves the train/test embeddings \n",
    "    '''\n",
    "    \n",
    "    #split the data into train/test - 80-20 split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split, random_state=42, shuffle = True)\n",
    "\n",
    "    train_corpus = list(read_corpus(X_train))\n",
    "    test_corpus = list(read_corpus(X_test, tokens_only=True))\n",
    "    \n",
    "    # Train Doc2Vec model\n",
    "    # Change parameter values 'vector_size=300, min_count=2, epochs=10' to modify doc2vec training\n",
    "    Doc2vec(train_corpus,test_corpus)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    #Relevant news articles\n",
    "    rdata = pd.read_json('../../data/clean/relevant_news_10K.json')\n",
    "    #Irrelevant news articles\n",
    "    irdata = pd.read_json('../../data/clean/irrelevant_news_10K.json')\n",
    "\n",
    "    rdata['relevance']=1  #Relevant\n",
    "    irdata['relevance']=0 #Irrelevant\n",
    "    #Combine the data\n",
    "    data = rdata.append(irdata_json, ignore_index=True)\n",
    "\n",
    "    # Let's use content as our data\n",
    "    X = data['content']\n",
    "    Y = data['relevance']\n",
    "    \n",
    "    #specify parameter 'split' for modifying test set size, default is 20%\n",
    "    main(X,Y)\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
